{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d930abe5-4d5c-4572-8c7b-8df531e3a4cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Medical Orders Data Exploration\n",
    "Welcome! This notebook walks through an **exploratory analysis** of the `mma_fe_innovation.mma.medical_orders_silver` dataset, which contains medical order information from wearable device retailers.  \n",
    "\n",
    "## Objectives\n",
    "The goal of this exercise is to:\n",
    "\n",
    "1. **Understand the dataset**\n",
    "   - Examine structure, data types, key columns, and general content.\n",
    "   - Identify missing values, anomalies, and potential outliers.\n",
    "\n",
    "2. **Assess data quality**\n",
    "   - Ensure that the dataset is reliable for analysis and AI modeling.\n",
    "   - Detect and flag unusual values that could affect downstream results.\n",
    "\n",
    "3. **Identify patterns and relationships**\n",
    "   - Aggregate orders by retailer, device, and combinations.\n",
    "   - Highlight popular products and trends that can inform business decisions.\n",
    "\n",
    "4. **Set the stage for AI & predictive analytics**\n",
    "   - Prepare a clean, well-understood dataset for building predictive models.\n",
    "   - Support tasks like high-demand device prediction, anomaly detection, and recommendation systems.\n",
    "\n",
    "> **Why this matters:** A thorough exploration builds a solid foundation for AI insights. It ensures that any models or predictions are grounded in a deep understanding of the real-world data, reducing risk and improving reliability.\n",
    "\n",
    "## How to use this notebook\n",
    "- Run each section sequentially.  \n",
    "- Use the visualizations and summaries to guide understanding.  \n",
    "- Identify any issues that need cleaning or further investigation before modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a2c347c-229e-48e4-926d-cec328e91c67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Load the Table & Peek at the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4008aea2-8b01-41a4-ba54-c300112e22d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# load\n",
    "df = spark.table(\"mma_fe_innovation.mma.medical_orders_silver\")\n",
    "\n",
    "# schema + quick peek\n",
    "df.printSchema()\n",
    "display(df.limit(200))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84459996-f9d5-4118-b29e-8ff884ee8639",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2. Dataset Overview & Null Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "451ef664-76ba-4893-b95f-e85fd8d91425",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "summary = df.select(\n",
    "    F.count(\"*\").alias(\"rows\"),\n",
    "    F.countDistinct(\"order_id\").alias(\"distinct_order_id\"),\n",
    "    F.countDistinct(\"retailer_name\").alias(\"distinct_retailer\"),\n",
    "    F.countDistinct(\"device_name\").alias(\"distinct_device\"),\n",
    "    F.min(\"order_date\").alias(\"min_order_date\"),\n",
    "    F.max(\"order_date\").alias(\"max_order_date\")\n",
    ")\n",
    "display(summary)\n",
    "\n",
    "nulls = df.select([F.sum(F.when(F.col(c).isNull() | (F.col(c) == \"\"), 1).otherwise(0)).alias(c + \"_nulls\")\n",
    "                   for c in df.columns])\n",
    "display(nulls)\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "summary = df.select(\n",
    "    F.count(\"*\").alias(\"rows\"),\n",
    "    F.countDistinct(\"order_id\").alias(\"distinct_order_id\"),\n",
    "    F.countDistinct(\"retailer_name\").alias(\"distinct_retailer\"),\n",
    "    F.countDistinct(\"device_name\").alias(\"distinct_device\"),\n",
    "    F.min(\"order_date\").alias(\"min_order_date\"),\n",
    "    F.max(\"order_date\").alias(\"max_order_date\")\n",
    ")\n",
    "display(summary)\n",
    "\n",
    "nulls = df.select([F.sum(F.when(F.col(c).isNull() | (F.col(c) == \"\"), 1).otherwise(0)).alias(c + \"_nulls\")\n",
    "                   for c in df.columns])\n",
    "display(nulls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "195dd3e8-b9fa-4b75-b24d-7af917350392",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##3. Timestamp Handling & Derived Date Columns\n",
    "Dates are central to exploring time-series behavior. Here we:\n",
    " - Parse order_date as a timestamp\n",
    " - Derive year, month, and week columns\n",
    " - Flag any rows with invalid/unparsable dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fe25de9-f1ee-4f47-ab71-3c3dc0ab9dd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_time = (\n",
    "    df.withColumn(\"order_ts\", F.to_timestamp(\"order_date\"))\n",
    "      .withColumn(\"order_date_only\", F.to_date(\"order_ts\"))\n",
    "      .withColumn(\"order_year\", F.year(\"order_ts\"))\n",
    "      .withColumn(\"order_month\", F.date_format(\"order_ts\", \"yyyy-MM\"))\n",
    "      .withColumn(\"order_week\", F.concat_ws(\n",
    "          \"-\",\n",
    "          F.year(\"order_ts\"),\n",
    "          F.format_string(\"%02d\", F.weekofyear(\"order_ts\"))\n",
    "      ))\n",
    ")\n",
    "\n",
    "bad_dates = df_time.filter(F.col(\"order_ts\").isNull()).limit(50)\n",
    "print(\"Unparsed dates:\", df_time.filter(F.col(\"order_ts\").isNull()).count())\n",
    "display(bad_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "780dad50-743b-4b34-97d4-f7671068b04d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#4. Quantity Distribution & Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a998be14-2542-46ff-a26b-15a32911d27f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.select(\"quantity\").describe())\n",
    "\n",
    "quantiles = df.approxQuantile(\"quantity\", [0.01,0.05,0.25,0.5,0.75,0.95,0.99,1.0], 0.001)\n",
    "print(dict(zip([\"p1\",\"p5\",\"p25\",\"p50\",\"p75\",\"p95\",\"p99\",\"max\"], quantiles)))\n",
    "\n",
    "non_positive = df.filter(F.col(\"quantity\") <= 0).limit(50)\n",
    "print(\"Non-positive quantities:\", df.filter(F.col(\"quantity\") <= 0).count())\n",
    "display(non_positive)\n",
    "\n",
    "display(df.orderBy(F.desc(\"quantity\")).limit(50))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1ef887d-01d5-41ca-a5fe-c7612ec9aaef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#5. Retailer and Device Popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36f05e42-8b9d-42d0-ba31-bc8bbf8e19ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "retailer_stats = df.groupBy(\"retailer_name\").agg(\n",
    "    F.count(\"*\").alias(\"rows\"),\n",
    "    F.countDistinct(\"order_id\").alias(\"distinct_orders\"),\n",
    "    F.sum(\"quantity\").alias(\"total_quantity\")\n",
    ").orderBy(F.desc(\"total_quantity\"))\n",
    "\n",
    "device_stats = df.groupBy(\"device_name\").agg(\n",
    "    F.count(\"*\").alias(\"rows\"),\n",
    "    F.countDistinct(\"order_id\").alias(\"distinct_orders\"),\n",
    "    F.sum(\"quantity\").alias(\"total_quantity\")\n",
    ").orderBy(F.desc(\"total_quantity\"))\n",
    "\n",
    "display(retailer_stats.limit(25))\n",
    "display(device_stats.limit(25))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e59d7575-0135-4c3a-a14a-4856e5e2ee26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#6. Retailer and Device Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e9be848-4aca-4dfb-a721-1a057cdb5466",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "combo = df.groupBy(\"retailer_name\", \"device_name\").agg(\n",
    "    F.count(\"*\").alias(\"rows\"),\n",
    "    F.sum(\"quantity\").alias(\"total_qty\")\n",
    ").orderBy(F.desc(\"total_qty\"))\n",
    "display(combo.limit(50))\n",
    "\n",
    "w = Window.partitionBy(\"retailer_name\").orderBy(F.desc(\"total_qty\"))\n",
    "top_per_retailer = combo.withColumn(\"rank\", F.row_number().over(w)).filter(F.col(\"rank\") <= 3).orderBy(\"retailer_name\", \"rank\")\n",
    "display(top_per_retailer)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_65bc13ea-276c-4905-a728-9fe2fb1780e2",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4640557381830671,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "data_exploration_final",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
