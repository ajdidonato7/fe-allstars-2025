{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0951120-52b0-4b11-bb20-dc059a4b58c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from datetime import datetime, timedelta\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For Databricks\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Initialize Spark session (already available in Databricks)\n",
    "spark = SparkSession.builder.appName(\"MedicalOrderPrediction\").getOrCreate()\n",
    "\n",
    "def load_and_prepare_data(sample_fraction=None, max_rows=100000):\n",
    "    \"\"\"Load the medical orders data from Databricks table and prepare it for modeling\"\"\"\n",
    "    \n",
    "    # Read data from Databricks table\n",
    "    df_spark = spark.table(\"mma_fe_innovation.mma.medical_orders_silver\")\n",
    "    \n",
    "    # Check the size first\n",
    "    total_count = df_spark.count()\n",
    "    print(f\"Total records in medical_orders_silver: {total_count:,}\")\n",
    "    \n",
    "    # Apply sampling or filtering if dataset is too large\n",
    "    if total_count > max_rows:\n",
    "        print(f\"Dataset is large ({total_count:,} rows). Applying sampling/filtering...\")\n",
    "        \n",
    "        if sample_fraction is None:\n",
    "            # Calculate sample fraction to get approximately max_rows\n",
    "            sample_fraction = min(0.5, max_rows / total_count)\n",
    "        \n",
    "        print(f\"Sampling {sample_fraction:.3f} of the data...\")\n",
    "        df_spark = df_spark.sample(fraction=sample_fraction, seed=42)\n",
    "        \n",
    "        # Also limit by recent dates to get more relevant data\n",
    "        df_spark = (df_spark\n",
    "                   .orderBy(F.desc(\"order_date\"))\n",
    "                   .limit(max_rows))\n",
    "    \n",
    "    # Convert to Pandas for easier manipulation\n",
    "    df = df_spark.toPandas()\n",
    "    \n",
    "    print(f\"Loaded {len(df):,} records for analysis\")\n",
    "    print(f\"Date range: {df['order_date'].min()} to {df['order_date'].max()}\")\n",
    "    print(f\"Unique devices: {df['device_name'].nunique()}\")\n",
    "    print(f\"Unique retailers: {df['retailer_name'].nunique()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def engineer_features(df, target_device=None, min_orders=5):\n",
    "    \"\"\"Create features for the linear regression model\"\"\"\n",
    "    \n",
    "    # Convert order_date to datetime\n",
    "    df['order_date'] = pd.to_datetime(df['order_date'])\n",
    "    \n",
    "    # If no specific device is specified, find devices with enough data\n",
    "    if target_device is None:\n",
    "        device_counts = df['device_name'].value_counts()\n",
    "        # Filter devices with at least min_orders\n",
    "        valid_devices = device_counts[device_counts >= min_orders]\n",
    "        \n",
    "        if len(valid_devices) == 0:\n",
    "            print(f\"No devices found with at least {min_orders} orders. Using device with most orders.\")\n",
    "            target_device = device_counts.index[0]\n",
    "        else:\n",
    "            target_device = valid_devices.index[0]\n",
    "            \n",
    "        print(f\"Selected device: '{target_device}' ({device_counts[target_device]} orders)\")\n",
    "    \n",
    "    # Filter for the target device\n",
    "    device_df = df[df['device_name'] == target_device].copy()\n",
    "    \n",
    "    if len(device_df) < min_orders:\n",
    "        print(f\"Warning: Only {len(device_df)} orders found for device '{target_device}'. Model may not be reliable.\")\n",
    "    \n",
    "    # Create time-based features\n",
    "    device_df['year'] = device_df['order_date'].dt.year\n",
    "    device_df['month'] = device_df['order_date'].dt.month\n",
    "    device_df['day_of_year'] = device_df['order_date'].dt.dayofyear\n",
    "    device_df['days_since_start'] = (device_df['order_date'] - device_df['order_date'].min()).dt.days\n",
    "    device_df['quarter'] = device_df['order_date'].dt.quarter\n",
    "    device_df['is_weekend'] = device_df['order_date'].dt.weekday >= 5\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    retailer_encoder = LabelEncoder()\n",
    "    device_df['retailer_encoded'] = retailer_encoder.fit_transform(device_df['retailer_name'])\n",
    "    \n",
    "    # Create lag features (previous order quantities)\n",
    "    device_df = device_df.sort_values('order_date')\n",
    "    device_df['quantity_lag1'] = device_df['quantity'].shift(1)\n",
    "    device_df['quantity_lag2'] = device_df['quantity'].shift(2)\n",
    "    device_df['quantity_rolling_mean_3'] = device_df['quantity'].rolling(window=3, min_periods=1).mean().shift(1)\n",
    "    \n",
    "    # Calculate days since last order for this device\n",
    "    device_df['days_since_last_order'] = device_df['order_date'].diff().dt.days\n",
    "    \n",
    "    # Drop rows with NaN values created by lag features (keep at least some data)\n",
    "    device_df = device_df.dropna(subset=['quantity_lag1'])\n",
    "    \n",
    "    return device_df, target_device, retailer_encoder\n",
    "\n",
    "def prepare_features_target(df):\n",
    "    \"\"\"Prepare feature matrix X and target vector y\"\"\"\n",
    "    \n",
    "    feature_cols = [\n",
    "        'year', 'month', 'day_of_year', 'days_since_start', 'quarter',\n",
    "        'is_weekend', 'retailer_encoded', 'quantity_lag1', 'quantity_lag2',\n",
    "        'quantity_rolling_mean_3', 'days_since_last_order'\n",
    "    ]\n",
    "    \n",
    "    # Handle any remaining NaN values\n",
    "    df[feature_cols] = df[feature_cols].fillna(df[feature_cols].mean())\n",
    "    \n",
    "    X = df[feature_cols].values\n",
    "    y = df['quantity'].values\n",
    "    \n",
    "    return X, y, feature_cols\n",
    "\n",
    "def train_model_with_mlflow(X, y, feature_names, target_device, test_size=0.2, random_state=42):\n",
    "    \"\"\"Train linear regression model and log with MLflow\"\"\"\n",
    "    \n",
    "    # Start MLflow run\n",
    "    with mlflow.start_run(run_name=f\"medical_order_prediction_{target_device[:30]}\") as run:\n",
    "        \n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"target_device\", target_device)\n",
    "        mlflow.log_param(\"test_size\", test_size)\n",
    "        mlflow.log_param(\"random_state\", random_state)\n",
    "        mlflow.log_param(\"n_features\", len(feature_names))\n",
    "        mlflow.log_param(\"n_samples\", len(X))\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "        train_r2 = r2_score(y_train, y_pred_train)\n",
    "        test_r2 = r2_score(y_test, y_pred_test)\n",
    "        train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "        test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metrics({\n",
    "            \"train_rmse\": train_rmse,\n",
    "            \"test_rmse\": test_rmse,\n",
    "            \"train_r2\": train_r2,\n",
    "            \"test_r2\": test_r2,\n",
    "            \"train_mae\": train_mae,\n",
    "            \"test_mae\": test_mae\n",
    "        })\n",
    "        \n",
    "        # Log feature importance\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'coefficient': model.coef_,\n",
    "            'abs_coefficient': np.abs(model.coef_)\n",
    "        }).sort_values('abs_coefficient', ascending=False)\n",
    "        \n",
    "        print(\"Feature Importance:\")\n",
    "        print(feature_importance)\n",
    "        \n",
    "        # Log feature importance as artifact\n",
    "        feature_importance.to_csv(\"feature_importance.csv\", index=False)\n",
    "        mlflow.log_artifact(\"feature_importance.csv\")\n",
    "        \n",
    "        # Create model signature\n",
    "        signature = infer_signature(X_train, y_pred_train)\n",
    "        \n",
    "        # Log model\n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=model,\n",
    "            artifact_path=\"model\",\n",
    "            signature=signature,\n",
    "            input_example=X_train[:5]\n",
    "        )\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\nModel Performance for device: {target_device}\")\n",
    "        print(f\"Training RMSE: {train_rmse:.3f}\")\n",
    "        print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "        print(f\"Training R²: {train_r2:.3f}\")\n",
    "        print(f\"Test R²: {test_r2:.3f}\")\n",
    "        print(f\"Training MAE: {train_mae:.3f}\")\n",
    "        print(f\"Test MAE: {test_mae:.3f}\")\n",
    "        \n",
    "        return model, run.info.run_id\n",
    "\n",
    "def predict_future_quantity(model, last_order_data, days_ahead=30, retailer_name=None):\n",
    "    \"\"\"Predict quantity for a future date, optionally for a specific retailer\"\"\"\n",
    "    \n",
    "    # Create future date features\n",
    "    future_date = last_order_data['order_date'].max() + timedelta(days=days_ahead)\n",
    "    \n",
    "    # Handle retailer selection\n",
    "    if retailer_name is not None:\n",
    "        # Use specific retailer if provided\n",
    "        retailer_encoded = last_order_data[\n",
    "            last_order_data['retailer_name'] == retailer_name\n",
    "        ]['retailer_encoded']\n",
    "        \n",
    "        if len(retailer_encoded) == 0:\n",
    "            print(f\"Warning: Retailer '{retailer_name}' not found in historical data. Using most common retailer.\")\n",
    "            retailer_encoded = last_order_data['retailer_encoded'].mode()[0]\n",
    "        else:\n",
    "            retailer_encoded = retailer_encoded.iloc[0]\n",
    "    else:\n",
    "        # Use most common retailer as default\n",
    "        retailer_encoded = last_order_data['retailer_encoded'].mode()[0]\n",
    "    \n",
    "    future_features = {\n",
    "        'year': future_date.year,\n",
    "        'month': future_date.month,\n",
    "        'day_of_year': future_date.timetuple().tm_yday,\n",
    "        'days_since_start': (future_date - last_order_data['order_date'].min()).days,\n",
    "        'quarter': (future_date.month - 1) // 3 + 1,\n",
    "        'is_weekend': future_date.weekday() >= 5,\n",
    "        'retailer_encoded': retailer_encoded,\n",
    "        'quantity_lag1': last_order_data['quantity'].iloc[-1],\n",
    "        'quantity_lag2': last_order_data['quantity'].iloc[-2] if len(last_order_data) > 1 else last_order_data['quantity'].iloc[-1],\n",
    "        'quantity_rolling_mean_3': last_order_data['quantity'].tail(3).mean(),\n",
    "        'days_since_last_order': days_ahead\n",
    "    }\n",
    "    \n",
    "    # Convert to array in the same order as training features\n",
    "    feature_array = np.array([[\n",
    "        future_features['year'], future_features['month'], \n",
    "        future_features['day_of_year'], future_features['days_since_start'],\n",
    "        future_features['quarter'], future_features['is_weekend'],\n",
    "        future_features['retailer_encoded'], future_features['quantity_lag1'],\n",
    "        future_features['quantity_lag2'], future_features['quantity_rolling_mean_3'],\n",
    "        future_features['days_since_last_order']\n",
    "    ]])\n",
    "    \n",
    "    prediction = model.predict(feature_array)[0]\n",
    "    return max(0, round(prediction))  # Ensure non-negative integer\n",
    "\n",
    "def predict_for_multiple_retailers(model, last_order_data, retailer_encoder, days_ahead=30, top_n=5):\n",
    "    \"\"\"Predict quantities for multiple retailers\"\"\"\n",
    "    \n",
    "    # Get top retailers by order frequency\n",
    "    retailer_counts = last_order_data['retailer_name'].value_counts().head(top_n)\n",
    "    \n",
    "    print(f\"\\nPredictions for top {top_n} retailers in {days_ahead} days:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    predictions = {}\n",
    "    \n",
    "    for retailer_name in retailer_counts.index:\n",
    "        try:\n",
    "            prediction = predict_future_quantity(\n",
    "                model, last_order_data, days_ahead, retailer_name\n",
    "            )\n",
    "            predictions[retailer_name] = prediction\n",
    "            historical_avg = last_order_data[\n",
    "                last_order_data['retailer_name'] == retailer_name\n",
    "            ]['quantity'].mean()\n",
    "            \n",
    "            print(f\"{retailer_name[:40]:<40} | Predicted: {prediction:>3} | Historical Avg: {historical_avg:>5.1f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error predicting for {retailer_name}: {e}\")\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    \n",
    "    print(\"Using default MLflow experiment\")\n",
    "    \n",
    "    # Load data with size management\n",
    "    df = load_and_prepare_data(max_rows=50000)\n",
    "    \n",
    "    # You can specify a particular device here, or let it auto-select the most frequent one\n",
    "    target_device = None  # Will auto-select most frequent device\n",
    "    \n",
    "    # Engineer features and get device_df before analysis\n",
    "    device_df, target_device, retailer_encoder = engineer_features(df, target_device)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RETAILER ANALYSIS FOR: {target_device}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    retailer_analysis = device_df.groupby('retailer_name').agg({\n",
    "        'quantity': ['count', 'mean', 'sum', 'std'],\n",
    "        'order_date': ['min', 'max']\n",
    "    }).round(2)\n",
    "    \n",
    "    retailer_analysis.columns = ['Order_Count', 'Avg_Quantity', 'Total_Quantity', 'Std_Quantity', 'First_Order', 'Last_Order']\n",
    "    retailer_analysis = retailer_analysis.sort_values('Order_Count', ascending=False)\n",
    "    \n",
    "    print(\"Top retailers for this device:\")\n",
    "    print(retailer_analysis.head(10))\n",
    "    \n",
    "    if len(device_df) < 5:\n",
    "        print(\"Not enough data to build a reliable model. Try selecting a different device.\")\n",
    "        return\n",
    "    \n",
    "    X, y, feature_names = prepare_features_target(device_df)\n",
    "    model, run_id = train_model_with_mlflow(X, y, feature_names, target_device)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"PREDICTION SCENARIOS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    future_quantity = predict_future_quantity(model, device_df, days_ahead=30)\n",
    "    most_common_retailer = device_df['retailer_name'].mode()[0]\n",
    "    print(f\"\\nDefault prediction (using most common retailer '{most_common_retailer}'):\")\n",
    "    print(f\"Predicted quantity for {target_device} in 30 days: {future_quantity}\")\n",
    "    \n",
    "    retailer_predictions = predict_for_multiple_retailers(\n",
    "        model, device_df, retailer_encoder, days_ahead=30, top_n=5\n",
    "    )\n",
    "    \n",
    "    with mlflow.start_run(run_id=run_id):\n",
    "        mlflow.log_param(\"future_prediction_days\", 30)\n",
    "        mlflow.log_metric(\"predicted_future_quantity_default\", future_quantity)\n",
    "        for retailer, prediction in retailer_predictions.items():\n",
    "            safe_retailer_name = retailer.replace(\" \", \"_\").replace(\",\", \"\")[:20]\n",
    "            mlflow.log_metric(f\"prediction_{safe_retailer_name}\", prediction)\n",
    "    \n",
    "    print(f\"\\nMLflow run ID: {run_id}\")\n",
    "    print(\"Check the MLflow UI in Databricks for detailed results and model artifacts.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# Additional helper function to get top devices by order frequency\n",
    "def get_top_devices(limit=10, max_rows=10000):\n",
    "    \"\"\"Helper function to see the most frequently ordered devices\"\"\"\n",
    "    df_spark = spark.table(\"mma_fe_innovation.mma.medical_orders_silver\")\n",
    "    \n",
    "    # Sample data if too large\n",
    "    total_count = df_spark.count()\n",
    "    if total_count > max_rows:\n",
    "        sample_fraction = max_rows / total_count\n",
    "        df_spark = df_spark.sample(fraction=sample_fraction, seed=42)\n",
    "        print(f\"Sampled {sample_fraction:.3f} of data for device analysis\")\n",
    "    \n",
    "    top_devices = (df_spark\n",
    "                   .groupBy(\"device_name\")\n",
    "                   .count()\n",
    "                   .orderBy(F.desc(\"count\"))\n",
    "                   .limit(limit))\n",
    "    \n",
    "    print(\"Top devices by order frequency:\")\n",
    "    top_devices.show(truncate=False)\n",
    "    return top_devices\n",
    "\n",
    "# Alternative approach: Work entirely in Spark without converting to Pandas\n",
    "def analyze_device_spark_only(device_name, limit_records=1000):\n",
    "    \"\"\"Analyze a specific device using only Spark operations\"\"\"\n",
    "    df_spark = spark.table(\"mma_fe_innovation.mma.medical_orders_silver\")\n",
    "    \n",
    "    device_orders = (df_spark\n",
    "                    .filter(F.col(\"device_name\") == device_name)\n",
    "                    .orderBy(F.desc(\"order_date\"))\n",
    "                    .limit(limit_records))  # Limit records for memory safety\n",
    "    \n",
    "    print(f\"\\nAnalysis for device: {device_name}\")\n",
    "    print(f\"Total orders (limited to {limit_records}): {device_orders.count()}\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    stats = device_orders.agg(\n",
    "        F.avg(\"quantity\").alias(\"avg_quantity\"),\n",
    "        F.min(\"quantity\").alias(\"min_quantity\"), \n",
    "        F.max(\"quantity\").alias(\"max_quantity\"),\n",
    "        F.min(\"order_date\").alias(\"earliest_order\"),\n",
    "        F.max(\"order_date\").alias(\"latest_order\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f\"Quantity stats - Avg: {stats['avg_quantity']:.1f}, Min: {stats['min_quantity']}, Max: {stats['max_quantity']}\")\n",
    "    print(f\"Date range: {stats['earliest_order']} to {stats['latest_order']}\")\n",
    "    \n",
    "    return device_orders.toPandas()  # Only convert small filtered dataset\n",
    "\n",
    "# Uncomment the line below to see top devices before running the model\n",
    "# get_top_devices()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "linear_reg_order_prediction",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
